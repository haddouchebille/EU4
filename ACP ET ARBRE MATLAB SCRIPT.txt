
% chargement de la base
load  base1;

X=base1;
varargin=X;

% Analyse en composantes principales
close all;
CPX=[];
[nb_ind,nb_var]=size(X);	 
				             
moy=mean(X);					
ecart=std(X,1);					

Xc=(X-ones(nb_ind,1)*moy);			

Xr=Xc./(ones(nb_ind,1)*ecart);			
save Xr Xr;
Corrx=corrcoef(Xr);		
save Corrx Corrx

% Valeurs propres et vecteurs propres de la matrice des corrélation
[eigenvec, eigenvalx]=eig(Corrx);
eigenvalx=sum(eigenvalx)	;		
save eigenvalx eigenvalx

%vecteurs propres
[rien,indexes]=sort(eigenvalx);	%classement en ordre croissant 		
lambda=eigenvalx(:,indexes)			
ux=eigenvec(:,indexes);				
save ux ux
CPX=Xr*ux;					
Gx=(ones(nb_var,1)*sqrt(lambda)).*ux;		
save Gx Gx;
TAUX=100*lambda'/sum(lambda);
save CPX CPX

%gx= (TAUX > min);
%save gx gx;
%sc = sum(gx);
%CP1x = ux(:,1:);
%save CP1x CP1x;
%Ptx = Xr*CP1x;
%x=Ptx;
%save Ptx Ptx ;% matrice ACP compressé
basx =Ptx;
 n=['thinec' e ];save(n,'basx');
 
   save CPX CPX;%matrice ACP complete
    cpf=CPX(:,1:2);
 figure (3),biplot(cpf);
  save CPX CPX;%matrice ACP complete
    cpf=CPX(:,1:3);
 figure(1),biplot(cpf);
 cpf1=Ptx(:,1:3);
 figure(2),biplot(cpf1);
 
        
        X = X(all(~isnan(X),2),:);
      [coefs,score] = princomp(zscore(X));        
      vlabs = {'A1','A2','A3','A4','A5'};
     biplot(coefs(:,1:3), 'scores',score(:,1:3));
       hold on
        t=-pi:.1:pi+.1;						%trace un cercle de rayon 1 sur la figure
 plot (sin(t),cos(t))					%cercle des correlations
axis('equal')
hold off











function [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% returns a trained classifier and its accuracy. This code recreates the
% classification model trained in Classification Learner app. Use the
% generated code to automate training the same model with new data, or to
% learn how to programmatically train models.
%
%  Input:
%      trainingData: a table containing the same predictor and response
%       columns as imported into the app.
%
%  Output:
%      trainedClassifier: a struct containing the trained classifier. The
%       struct contains various fields with information about the trained
%       classifier.
%
%      trainedClassifier.predictFcn: a function to make predictions on new
%       data.
%
%      validationAccuracy: a double containing the accuracy in percent. In
%       the app, the History list displays this overall accuracy score for
%       each model.
%
% Use the code to train the model with new data. To retrain your
% classifier, call the function from the command line with your original
% data or new data as the input argument trainingData.
%
% For example, to retrain a classifier trained with the original data set
% T, enter:
%   [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
% To make predictions with the returned 'trainedClassifier' on new data T2,
% use
%   yfit = trainedClassifier.predictFcn(T2)
%
% T2 must be a table containing at least the same predictor columns as used
% during training. For details, enter:
%   trainedClassifier.HowToPredict

% Auto-generated by MATLAB on 17-Dec-2017 22:34:18


% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'TrainingData1', 'TrainingData2', 'TrainingData3', 'TrainingData4', 'TrainingData5', 'TrainingData6', 'TrainingData7', 'TrainingData8', 'TrainingData9', 'TrainingData10', 'TrainingData11', 'TrainingData12', 'TrainingData13', 'TrainingData14', 'TrainingData15', 'TrainingData16', 'TrainingData17', 'TrainingData18', 'TrainingData19', 'TrainingData20', 'TrainingData21', 'TrainingData22', 'TrainingData23', 'TrainingData24', 'TrainingData25', 'TrainingData26', 'TrainingData27', 'TrainingData28', 'TrainingData29', 'TrainingData30', 'TrainingData31', 'TrainingData32', 'TrainingData33', 'TrainingData34', 'TrainingData35', 'TrainingData36', 'TrainingData37', 'TrainingData38', 'TrainingData39', 'TrainingData40', 'TrainingData41', 'TrainingData42', 'TrainingData43', 'TrainingData44', 'TrainingData45', 'TrainingData46', 'TrainingData47', 'TrainingData48', 'TrainingData49', 'TrainingData50', 'TrainingData51', 'TrainingData52', 'TrainingData53', 'TrainingData54', 'TrainingData55', 'TrainingData56', 'TrainingData57', 'TrainingData58', 'TrainingData59', 'TrainingData60', 'TrainingData61', 'TrainingData62', 'TrainingData63', 'TrainingData64', 'TrainingData65', 'TrainingData66', 'TrainingData67', 'TrainingData68', 'TrainingData69', 'TrainingData70', 'TrainingData71', 'TrainingData72', 'TrainingData73', 'TrainingData74', 'TrainingData75', 'TrainingData76', 'TrainingData77', 'TrainingData78', 'TrainingData79', 'TrainingData80', 'TrainingData81', 'TrainingData82', 'TrainingData83', 'TrainingData84', 'TrainingData85', 'TrainingData86', 'TrainingData87', 'TrainingData88', 'TrainingData89', 'TrainingData90', 'TrainingData91', 'TrainingData92', 'TrainingData93', 'TrainingData94', 'TrainingData95', 'TrainingData96', 'TrainingData97', 'TrainingData98', 'TrainingData99', 'TrainingData100', 'TrainingData101', 'TrainingData102', 'TrainingData103', 'TrainingData104', 'TrainingData105', 'TrainingData106', 'TrainingData107', 'TrainingData108', 'TrainingData109', 'TrainingData110', 'TrainingData111', 'TrainingData112', 'TrainingData113', 'TrainingData114', 'TrainingData115', 'TrainingData116', 'TrainingData117', 'TrainingData118', 'TrainingData119', 'TrainingData120', 'TrainingData121', 'TrainingData122', 'TrainingData123', 'TrainingData124', 'TrainingData125', 'TrainingData126', 'TrainingData127', 'TrainingData128', 'TrainingData129', 'TrainingData130', 'TrainingData131', 'TrainingData132', 'TrainingData133', 'TrainingData134', 'TrainingData135', 'TrainingData136', 'TrainingData137', 'TrainingData138', 'TrainingData139', 'TrainingData140', 'TrainingData141', 'TrainingData142', 'TrainingData143', 'TrainingData144', 'TrainingData145', 'TrainingData146', 'TrainingData147', 'TrainingData148', 'TrainingData149', 'TrainingData150', 'TrainingData151', 'TrainingData152', 'TrainingData153', 'TrainingData154', 'TrainingData155', 'TrainingData156', 'TrainingData157', 'TrainingData158', 'TrainingData159', 'TrainingData160', 'TrainingData161', 'TrainingData162', 'TrainingData163', 'TrainingData164', 'TrainingData165', 'TrainingData166', 'TrainingData167', 'TrainingData168', 'TrainingData169', 'TrainingData170', 'TrainingData171', 'TrainingData172', 'TrainingData173', 'TrainingData174', 'TrainingData175', 'TrainingData176', 'TrainingData177', 'TrainingData178', 'TrainingData179', 'TrainingData180', 'TrainingData181', 'TrainingData182', 'TrainingData183', 'TrainingData184', 'TrainingData185', 'TrainingData186', 'TrainingData187', 'TrainingData188', 'TrainingData189', 'TrainingData190', 'TrainingData191', 'TrainingData192', 'TrainingData193', 'TrainingData194', 'TrainingData195', 'TrainingData196', 'TrainingData197', 'TrainingData198', 'TrainingData199', 'TrainingData200', 'TrainingData201', 'TrainingData202', 'TrainingData203', 'TrainingData204', 'TrainingData205', 'TrainingData206', 'TrainingData207', 'TrainingData208', 'TrainingData209', 'TrainingData210', 'TrainingData211', 'TrainingData212', 'TrainingData213', 'TrainingData214', 'TrainingData215', 'TrainingData216', 'TrainingData217', 'TrainingData218', 'TrainingData219', 'TrainingData220', 'TrainingData221', 'TrainingData222', 'TrainingData223', 'TrainingData224', 'TrainingData225', 'TrainingData226', 'TrainingData227', 'TrainingData228', 'TrainingData229', 'TrainingData230', 'TrainingData231', 'TrainingData232', 'TrainingData233', 'TrainingData234', 'TrainingData235', 'TrainingData236', 'TrainingData237', 'TrainingData238', 'TrainingData239', 'TrainingData240', 'TrainingData241', 'TrainingData242', 'TrainingData243', 'TrainingData244', 'TrainingData245', 'TrainingData246', 'TrainingData247', 'TrainingData248', 'TrainingData249', 'TrainingData250'};
predictors = inputTable(:, predictorNames);
response = inputTable.TrainingData251;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
classificationTree = fitctree(...
    predictors, ...
    response, ...
    'SplitCriterion', 'gdi', ...
    'MaxNumSplits', 4, ...
    'Surrogate', 'off', ...
    'ClassNames', [0; 1]);

% Create the result struct with predict function
predictorExtractionFcn = @(t) t(:, predictorNames);
treePredictFcn = @(x) predict(classificationTree, x);
trainedClassifier.predictFcn = @(x) treePredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.RequiredVariables = {'TrainingData1', 'TrainingData2', 'TrainingData3', 'TrainingData4', 'TrainingData5', 'TrainingData6', 'TrainingData7', 'TrainingData8', 'TrainingData9', 'TrainingData10', 'TrainingData11', 'TrainingData12', 'TrainingData13', 'TrainingData14', 'TrainingData15', 'TrainingData16', 'TrainingData17', 'TrainingData18', 'TrainingData19', 'TrainingData20', 'TrainingData21', 'TrainingData22', 'TrainingData23', 'TrainingData24', 'TrainingData25', 'TrainingData26', 'TrainingData27', 'TrainingData28', 'TrainingData29', 'TrainingData30', 'TrainingData31', 'TrainingData32', 'TrainingData33', 'TrainingData34', 'TrainingData35', 'TrainingData36', 'TrainingData37', 'TrainingData38', 'TrainingData39', 'TrainingData40', 'TrainingData41', 'TrainingData42', 'TrainingData43', 'TrainingData44', 'TrainingData45', 'TrainingData46', 'TrainingData47', 'TrainingData48', 'TrainingData49', 'TrainingData50', 'TrainingData51', 'TrainingData52', 'TrainingData53', 'TrainingData54', 'TrainingData55', 'TrainingData56', 'TrainingData57', 'TrainingData58', 'TrainingData59', 'TrainingData60', 'TrainingData61', 'TrainingData62', 'TrainingData63', 'TrainingData64', 'TrainingData65', 'TrainingData66', 'TrainingData67', 'TrainingData68', 'TrainingData69', 'TrainingData70', 'TrainingData71', 'TrainingData72', 'TrainingData73', 'TrainingData74', 'TrainingData75', 'TrainingData76', 'TrainingData77', 'TrainingData78', 'TrainingData79', 'TrainingData80', 'TrainingData81', 'TrainingData82', 'TrainingData83', 'TrainingData84', 'TrainingData85', 'TrainingData86', 'TrainingData87', 'TrainingData88', 'TrainingData89', 'TrainingData90', 'TrainingData91', 'TrainingData92', 'TrainingData93', 'TrainingData94', 'TrainingData95', 'TrainingData96', 'TrainingData97', 'TrainingData98', 'TrainingData99', 'TrainingData100', 'TrainingData101', 'TrainingData102', 'TrainingData103', 'TrainingData104', 'TrainingData105', 'TrainingData106', 'TrainingData107', 'TrainingData108', 'TrainingData109', 'TrainingData110', 'TrainingData111', 'TrainingData112', 'TrainingData113', 'TrainingData114', 'TrainingData115', 'TrainingData116', 'TrainingData117', 'TrainingData118', 'TrainingData119', 'TrainingData120', 'TrainingData121', 'TrainingData122', 'TrainingData123', 'TrainingData124', 'TrainingData125', 'TrainingData126', 'TrainingData127', 'TrainingData128', 'TrainingData129', 'TrainingData130', 'TrainingData131', 'TrainingData132', 'TrainingData133', 'TrainingData134', 'TrainingData135', 'TrainingData136', 'TrainingData137', 'TrainingData138', 'TrainingData139', 'TrainingData140', 'TrainingData141', 'TrainingData142', 'TrainingData143', 'TrainingData144', 'TrainingData145', 'TrainingData146', 'TrainingData147', 'TrainingData148', 'TrainingData149', 'TrainingData150', 'TrainingData151', 'TrainingData152', 'TrainingData153', 'TrainingData154', 'TrainingData155', 'TrainingData156', 'TrainingData157', 'TrainingData158', 'TrainingData159', 'TrainingData160', 'TrainingData161', 'TrainingData162', 'TrainingData163', 'TrainingData164', 'TrainingData165', 'TrainingData166', 'TrainingData167', 'TrainingData168', 'TrainingData169', 'TrainingData170', 'TrainingData171', 'TrainingData172', 'TrainingData173', 'TrainingData174', 'TrainingData175', 'TrainingData176', 'TrainingData177', 'TrainingData178', 'TrainingData179', 'TrainingData180', 'TrainingData181', 'TrainingData182', 'TrainingData183', 'TrainingData184', 'TrainingData185', 'TrainingData186', 'TrainingData187', 'TrainingData188', 'TrainingData189', 'TrainingData190', 'TrainingData191', 'TrainingData192', 'TrainingData193', 'TrainingData194', 'TrainingData195', 'TrainingData196', 'TrainingData197', 'TrainingData198', 'TrainingData199', 'TrainingData200', 'TrainingData201', 'TrainingData202', 'TrainingData203', 'TrainingData204', 'TrainingData205', 'TrainingData206', 'TrainingData207', 'TrainingData208', 'TrainingData209', 'TrainingData210', 'TrainingData211', 'TrainingData212', 'TrainingData213', 'TrainingData214', 'TrainingData215', 'TrainingData216', 'TrainingData217', 'TrainingData218', 'TrainingData219', 'TrainingData220', 'TrainingData221', 'TrainingData222', 'TrainingData223', 'TrainingData224', 'TrainingData225', 'TrainingData226', 'TrainingData227', 'TrainingData228', 'TrainingData229', 'TrainingData230', 'TrainingData231', 'TrainingData232', 'TrainingData233', 'TrainingData234', 'TrainingData235', 'TrainingData236', 'TrainingData237', 'TrainingData238', 'TrainingData239', 'TrainingData240', 'TrainingData241', 'TrainingData242', 'TrainingData243', 'TrainingData244', 'TrainingData245', 'TrainingData246', 'TrainingData247', 'TrainingData248', 'TrainingData249', 'TrainingData250'};
trainedClassifier.ClassificationTree = classificationTree;
trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017a.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'TrainingData1', 'TrainingData2', 'TrainingData3', 'TrainingData4', 'TrainingData5', 'TrainingData6', 'TrainingData7', 'TrainingData8', 'TrainingData9', 'TrainingData10', 'TrainingData11', 'TrainingData12', 'TrainingData13', 'TrainingData14', 'TrainingData15', 'TrainingData16', 'TrainingData17', 'TrainingData18', 'TrainingData19', 'TrainingData20', 'TrainingData21', 'TrainingData22', 'TrainingData23', 'TrainingData24', 'TrainingData25', 'TrainingData26', 'TrainingData27', 'TrainingData28', 'TrainingData29', 'TrainingData30', 'TrainingData31', 'TrainingData32', 'TrainingData33', 'TrainingData34', 'TrainingData35', 'TrainingData36', 'TrainingData37', 'TrainingData38', 'TrainingData39', 'TrainingData40', 'TrainingData41', 'TrainingData42', 'TrainingData43', 'TrainingData44', 'TrainingData45', 'TrainingData46', 'TrainingData47', 'TrainingData48', 'TrainingData49', 'TrainingData50', 'TrainingData51', 'TrainingData52', 'TrainingData53', 'TrainingData54', 'TrainingData55', 'TrainingData56', 'TrainingData57', 'TrainingData58', 'TrainingData59', 'TrainingData60', 'TrainingData61', 'TrainingData62', 'TrainingData63', 'TrainingData64', 'TrainingData65', 'TrainingData66', 'TrainingData67', 'TrainingData68', 'TrainingData69', 'TrainingData70', 'TrainingData71', 'TrainingData72', 'TrainingData73', 'TrainingData74', 'TrainingData75', 'TrainingData76', 'TrainingData77', 'TrainingData78', 'TrainingData79', 'TrainingData80', 'TrainingData81', 'TrainingData82', 'TrainingData83', 'TrainingData84', 'TrainingData85', 'TrainingData86', 'TrainingData87', 'TrainingData88', 'TrainingData89', 'TrainingData90', 'TrainingData91', 'TrainingData92', 'TrainingData93', 'TrainingData94', 'TrainingData95', 'TrainingData96', 'TrainingData97', 'TrainingData98', 'TrainingData99', 'TrainingData100', 'TrainingData101', 'TrainingData102', 'TrainingData103', 'TrainingData104', 'TrainingData105', 'TrainingData106', 'TrainingData107', 'TrainingData108', 'TrainingData109', 'TrainingData110', 'TrainingData111', 'TrainingData112', 'TrainingData113', 'TrainingData114', 'TrainingData115', 'TrainingData116', 'TrainingData117', 'TrainingData118', 'TrainingData119', 'TrainingData120', 'TrainingData121', 'TrainingData122', 'TrainingData123', 'TrainingData124', 'TrainingData125', 'TrainingData126', 'TrainingData127', 'TrainingData128', 'TrainingData129', 'TrainingData130', 'TrainingData131', 'TrainingData132', 'TrainingData133', 'TrainingData134', 'TrainingData135', 'TrainingData136', 'TrainingData137', 'TrainingData138', 'TrainingData139', 'TrainingData140', 'TrainingData141', 'TrainingData142', 'TrainingData143', 'TrainingData144', 'TrainingData145', 'TrainingData146', 'TrainingData147', 'TrainingData148', 'TrainingData149', 'TrainingData150', 'TrainingData151', 'TrainingData152', 'TrainingData153', 'TrainingData154', 'TrainingData155', 'TrainingData156', 'TrainingData157', 'TrainingData158', 'TrainingData159', 'TrainingData160', 'TrainingData161', 'TrainingData162', 'TrainingData163', 'TrainingData164', 'TrainingData165', 'TrainingData166', 'TrainingData167', 'TrainingData168', 'TrainingData169', 'TrainingData170', 'TrainingData171', 'TrainingData172', 'TrainingData173', 'TrainingData174', 'TrainingData175', 'TrainingData176', 'TrainingData177', 'TrainingData178', 'TrainingData179', 'TrainingData180', 'TrainingData181', 'TrainingData182', 'TrainingData183', 'TrainingData184', 'TrainingData185', 'TrainingData186', 'TrainingData187', 'TrainingData188', 'TrainingData189', 'TrainingData190', 'TrainingData191', 'TrainingData192', 'TrainingData193', 'TrainingData194', 'TrainingData195', 'TrainingData196', 'TrainingData197', 'TrainingData198', 'TrainingData199', 'TrainingData200', 'TrainingData201', 'TrainingData202', 'TrainingData203', 'TrainingData204', 'TrainingData205', 'TrainingData206', 'TrainingData207', 'TrainingData208', 'TrainingData209', 'TrainingData210', 'TrainingData211', 'TrainingData212', 'TrainingData213', 'TrainingData214', 'TrainingData215', 'TrainingData216', 'TrainingData217', 'TrainingData218', 'TrainingData219', 'TrainingData220', 'TrainingData221', 'TrainingData222', 'TrainingData223', 'TrainingData224', 'TrainingData225', 'TrainingData226', 'TrainingData227', 'TrainingData228', 'TrainingData229', 'TrainingData230', 'TrainingData231', 'TrainingData232', 'TrainingData233', 'TrainingData234', 'TrainingData235', 'TrainingData236', 'TrainingData237', 'TrainingData238', 'TrainingData239', 'TrainingData240', 'TrainingData241', 'TrainingData242', 'TrainingData243', 'TrainingData244', 'TrainingData245', 'TrainingData246', 'TrainingData247', 'TrainingData248', 'TrainingData249', 'TrainingData250'};
predictors = inputTable(:, predictorNames);
response = inputTable.TrainingData251;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Perform cross-validation
partitionedModel = crossval(trainedClassifier.ClassificationTree, 'KFold', 5);

% Compute validation predictions
[validationPredictions, validationScores] = kfoldPredict(partitionedModel);

% Compute validation accuracy
validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');
